{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini API intro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make smart decisions or predictions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# looks automatically after the key\n",
    "# one of GOOGLE-API_KEY and GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  Why did the data engineer break up with the data scientist? The data scientist kept saying, \"Just ETL it,\" but never explained the \"T.\"\n",
      "2.  What's a data engineer's favorite type of music? Pipeline breaks, because they love the sound of silence when everything finally runs.\n",
      "3.  How do you know a data engineer is having a bad day? Their data lake is more like a data swamp, and their Airflow DAGs are doing the tango.\n",
      "4.  A data engineer's favorite pick-up line: \"Is your ETL broken? Because you're *extracting* all my attention.\"\n",
      "5.  What's a data engineer's spirit animal? A coffee-fueled owl, always awake to fix pipelines that break at 3 AM.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ask_gemini(prompt, model = \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=prompt,\n",
    ")\n",
    "    \n",
    "    return response\n",
    "\n",
    "response = ask_gemini(\"Give me 5 data engineering jokes, structure it in short points\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# knows that GenerateContentRespinse is pydantic model\n",
    "# -> we can work with it in a OOP manner \n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HttpResponse(\n",
       "  headers=<dict len=11>\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(\n",
       "   content=Content(\n",
       "     parts=[\n",
       "       Part(\n",
       "         text=\"\"\"Here are 5 data engineering jokes, structured in short points:\n",
       " \n",
       " 1.  Why did the data engineer break up with the data scientist? The data scientist kept saying, \"Just ETL it,\" but never explained the \"T.\"\n",
       " 2.  What's a data engineer's favorite type of music? Pipeline breaks, because they love the sound of silence when everything finally runs.\n",
       " 3.  How do you know a data engineer is having a bad day? Their data lake is more like a data swamp, and their Airflow DAGs are doing the tango.\n",
       " 4.  A data engineer's favorite pick-up line: \"Is your ETL broken? Because you're *extracting* all my attention.\"\n",
       " 5.  What's a data engineer's spirit animal? A coffee-fueled owl, always awake to fix pipelines that break at 3 AM.\"\"\"\n",
       "       ),\n",
       "     ],\n",
       "     role='model'\n",
       "   ),\n",
       "   finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "   index=0\n",
       " )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "* basic unit of text for LLMs\n",
    "\n",
    "* can be as short as one character or as long as one word\n",
    "\n",
    "* tokens used for billing\n",
    "\n",
    "Gemini free tier \n",
    "\n",
    "* Requests per minute (RPM): 10\n",
    "  \n",
    "* Tokens per minute (TPM): 250 000\n",
    "  \n",
    "* Requests per day (RPD): 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=183,\n",
       "  prompt_token_count=14,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=14\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1462,\n",
       "  total_token_count=1659\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thinking is expensive \n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hyperparameter to allocate more compute for complex tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "1.  **Why did the Spark job break up with the Kafka topic?**\n",
      "    *   It kept getting *offset*.\n",
      "\n",
      "2.  **What's a data engineer's favorite type of music?**\n",
      "    *   *Stream* music, especially with good *ETL* tunes.\n",
      "\n",
      "3.  **A data engineer walks into a bar...**\n",
      "    *   ...and says, \"I'd like a beer, but first, can you tell me the schema of your tap?\"\n",
      "\n",
      "4.  **How do you know a data engineer is at your party?**\n",
      "    *   They're trying to normalize your guest list and suggesting a better partition key for the snacks.\n",
      "\n",
      "5.  **What did the data lake say to the data swamp?**\n",
      "    *   \"You need to get your act together; you're full of *unstructured problems*!\"\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "prompt = \"Give me 5 some data engineering jokes, structure it in short points\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=203,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=218\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hyperparameter to guide model behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OOP (Object-Oriented Programming)**\n",
      "\n",
      "*   Organizes code around \"objects\" that combine data (attributes) and behavior (methods).\n",
      "*   Key principles:\n",
      "    *   **Encapsulation:** Hiding internal data and exposing controlled access.\n",
      "    *   **Inheritance:** Creating new classes from existing ones, inheriting their properties.\n",
      "    *   **Polymorphism:**  Objects of different classes responding to the same method call in their own way.\n",
      "    *   **Abstraction:** Simplifying complex systems by modeling classes appropriate to the problem.\n",
      "\n",
      "**Dunder (Double Underscore) Methods**\n",
      "\n",
      "*   Special methods in Python that start and end with double underscores (e.g., `__init__`, `__str__`).\n",
      "*   Used for operator overloading, customization, and implementing built-in behaviors.\n",
      "*   Examples:\n",
      "    *   `__init__`:  Constructor, initializes object attributes.\n",
      "    *   `__str__`:  String representation of an object (for `print()` or `str()`).\n",
      "    *   `__repr__`:  Unambiguous string representation of an object.\n",
      "    *   `__add__`:  Defines behavior for the `+` operator.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert in python programming, you will always provide idiomatic code, i.e.\n",
    "pythonic code. So when you see my code or my question, be very critical, but answer\n",
    "in a SHORT and CONCISE way. Also be constructive to help me improve. \n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explain OOP and dunder methods.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=256,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=256\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=70,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=70\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=326\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.candidates_token_count = 256\n",
      "metadata.prompt_token_count = 70\n",
      "metadata.total_token_count = 326\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count = }\") # output\n",
    "print(f\"{metadata.prompt_token_count = }\") # prompt + system instruction\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "* controls randomness of output -> 'creative'\n",
    "\n",
    "its a hyperparameter that can be adjusted to influence the diversity and creativity of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes, its fluffy tail disappearing in a flash of white. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the mechanical threat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes, its fluffy tail disappearing in a flash of white. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the mechanical threat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched his nose, sensing the danger that lurked just beyond the brambles. With a flick of his cotton tail, he disappeared into the warren, leaving the hungry fox to sniff the empty air. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby, a gray rabbit with a perpetually twitching nose, hopped through the tall grass, finally discovering a field of vibrant carrots just as the sun began to dip below the horizon. With a joyful squeak, he began his feast, the orange glow of the carrots mirroring the happiness in his eyes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        # system_instruction=system_instruction\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal input\n",
    "\n",
    "input text and image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a charming, close-up photo of a fluffy, gray rabbit wearing a miniature white and black student graduation cap. A festive blue and yellow ribbon is draped over its back. The rabbit appears to be resting peacefully on a gray carpet with its eyes closed.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
